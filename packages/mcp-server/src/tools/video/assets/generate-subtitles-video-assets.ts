// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { isJqError, maybeFilter } from '@mux/mcp/filtering';
import { Metadata, asErrorResult, asTextContentResult } from '@mux/mcp/tools/types';

import { Tool } from '@modelcontextprotocol/sdk/types.js';
import Mux from '@mux/mux-node';

export const metadata: Metadata = {
  resource: 'video.assets',
  operation: 'write',
  tags: [],
  httpMethod: 'post',
  httpPath: '/video/v1/assets/{ASSET_ID}/tracks/{TRACK_ID}/generate-subtitles',
  operationId: 'generate-asset-track-subtitles',
};

export const tool: Tool = {
  name: 'generate_subtitles_video_assets',
  description:
    "When using this tool, always use the `jq_filter` parameter to reduce the response size and improve performance.\n\nOnly omit if you're sure you don't need the data.\n\nGenerates subtitles (captions) for a given audio track. [See docs for more information.](https://mux.com/docs/guides/add-autogenerated-captions-and-use-transcripts#retroactively-enable-auto-generated-captions)\n\n# Response Schema\n```json\n{\n  $ref: '#/$defs/asset_generate_subtitles_response',\n  $defs: {\n    asset_generate_subtitles_response: {\n      type: 'array',\n      items: {\n        $ref: '#/$defs/track'\n      }\n    },\n    track: {\n      type: 'object',\n      properties: {\n        id: {\n          type: 'string',\n          description: 'Unique identifier for the Track'\n        },\n        closed_captions: {\n          type: 'boolean',\n          description: 'Indicates the track provides Subtitles for the Deaf or Hard-of-hearing (SDH). This parameter is only set tracks where `type` is `text` and `text_type` is `subtitles`.'\n        },\n        duration: {\n          type: 'number',\n          description: 'The duration in seconds of the track media. This parameter is not set for `text` type tracks. This field is optional and may not be set. The top level `duration` field of an asset will always be set.'\n        },\n        language_code: {\n          type: 'string',\n          description: 'The language code value represents [BCP 47](https://tools.ietf.org/html/bcp47) specification compliant value. For example, `en` for English or `en-US` for the US version of English. This parameter is only set for `text` and `audio` track types.'\n        },\n        max_channel_layout: {\n          type: 'string',\n          description: 'Only set for the `audio` type track.'\n        },\n        max_channels: {\n          type: 'integer',\n          description: 'The maximum number of audio channels the track supports. Only set for the `audio` type track.'\n        },\n        max_frame_rate: {\n          type: 'number',\n          description: 'The maximum frame rate available for the track. Only set for the `video` type track. This field may return `-1` if the frame rate of the input cannot be reliably determined.'\n        },\n        max_height: {\n          type: 'integer',\n          description: 'The maximum height in pixels available for the track. Only set for the `video` type track.'\n        },\n        max_width: {\n          type: 'integer',\n          description: 'The maximum width in pixels available for the track. Only set for the `video` type track.'\n        },\n        name: {\n          type: 'string',\n          description: 'The name of the track containing a human-readable description. The HLS manifest will associate a subtitle `text` or `audio` track with this value. For example, the value should be \"English\" for a subtitle text track for the `language_code` value of `en-US`. This parameter is only set for `text` and `audio` track types.'\n        },\n        passthrough: {\n          type: 'string',\n          description: 'Arbitrary user-supplied metadata set for the track either when creating the asset or track. This parameter is only set for `text` type tracks. Max 255 characters.'\n        },\n        primary: {\n          type: 'boolean',\n          description: 'For an audio track, indicates that this is the primary audio track, ingested from the main input for this asset. The primary audio track cannot be deleted.'\n        },\n        status: {\n          type: 'string',\n          description: 'The status of the track. This parameter is only set for `text` type tracks.',\n          enum: [            'preparing',\n            'ready',\n            'errored',\n            'deleted'\n          ]\n        },\n        text_source: {\n          type: 'string',\n          description: 'The source of the text contained in a Track of type `text`. Valid `text_source`\\nvalues are listed below.\\n* `uploaded`: Tracks uploaded to Mux as caption or subtitle files using the Create Asset Track API.\\n* `embedded`: Tracks extracted from an embedded stream of CEA-608 closed captions.\\n* `generated_vod`: Tracks generated by automatic speech recognition on an on-demand asset.\\n* `generated_live`: Tracks generated by automatic speech recognition on a live stream configured with `generated_subtitles`. If an Asset has both `generated_live` and `generated_live_final` tracks that are `ready`, then only the `generated_live_final` track will be included during playback.\\n* `generated_live_final`: Tracks generated by automatic speech recognition on a live stream using `generated_subtitles`. The accuracy, timing, and formatting of these subtitles is improved compared to the corresponding `generated_live` tracks. However, `generated_live_final` tracks will not be available in `ready` status until the live stream ends. If an Asset has both `generated_live` and `generated_live_final` tracks that are `ready`, then only the `generated_live_final` track will be included during playback.',\n          enum: [            'uploaded',\n            'embedded',\n            'generated_live',\n            'generated_live_final',\n            'generated_vod'\n          ]\n        },\n        text_type: {\n          type: 'string',\n          description: 'This parameter is only set for `text` type tracks.',\n          enum: [            'subtitles'\n          ]\n        },\n        type: {\n          type: 'string',\n          description: 'The type of track',\n          enum: [            'video',\n            'audio',\n            'text'\n          ]\n        }\n      }\n    }\n  }\n}\n```",
  inputSchema: {
    type: 'object',
    properties: {
      ASSET_ID: {
        type: 'string',
      },
      TRACK_ID: {
        type: 'string',
      },
      generated_subtitles: {
        type: 'array',
        description: 'Generate subtitle tracks using automatic speech recognition with this configuration.',
        items: {
          type: 'object',
          properties: {
            language_code: {
              type: 'string',
              description: 'The language to generate subtitles in.',
              enum: [
                'en',
                'es',
                'it',
                'pt',
                'de',
                'fr',
                'pl',
                'ru',
                'nl',
                'ca',
                'tr',
                'sv',
                'uk',
                'no',
                'fi',
                'sk',
                'el',
                'cs',
                'hr',
                'da',
                'ro',
                'bg',
              ],
            },
            name: {
              type: 'string',
              description: 'A name for this subtitle track.',
            },
            passthrough: {
              type: 'string',
              description: 'Arbitrary metadata set for the subtitle track. Max 255 characters.',
            },
          },
        },
      },
      jq_filter: {
        type: 'string',
        title: 'jq Filter',
        description:
          'A jq filter to apply to the response to include certain fields. Consult the output schema in the tool description to see the fields that are available.\n\nFor example: to include only the `name` field in every object of a results array, you can provide ".results[].name".\n\nFor more information, see the [jq documentation](https://jqlang.org/manual/).',
      },
    },
    required: ['ASSET_ID', 'TRACK_ID', 'generated_subtitles'],
  },
  annotations: {},
};

export const handler = async (client: Mux, args: Record<string, unknown> | undefined) => {
  const { ASSET_ID, TRACK_ID, jq_filter, ...body } = args as any;
  try {
    return asTextContentResult(
      await maybeFilter(jq_filter, await client.video.assets.generateSubtitles(ASSET_ID, TRACK_ID, body)),
    );
  } catch (error) {
    if (isJqError(error)) {
      return asErrorResult(error.message);
    }
    throw error;
  }
};

export default { metadata, tool, handler };
